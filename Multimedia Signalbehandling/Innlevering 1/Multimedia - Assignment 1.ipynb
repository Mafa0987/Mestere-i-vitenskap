{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c20e6045",
   "metadata": {},
   "source": [
    "# TTT4135 - Assignment 1 - Martin Eggen & Martin Færevaag\n",
    "\n",
    "## Problem 1 Information theory and data compression\n",
    "\n",
    "This problem set addresses information theory and lossless data compression. The first\n",
    "set of problems are theoretical only:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3829b5",
   "metadata": {},
   "source": [
    "## 1a)\n",
    "**Find the entropy for the following distributions:**\n",
    "- $P(X=x) = 1/2$ for  $X\\in \\{0,1\\}$\n",
    "- $P(X=0)=0, P(X=x) = 1/2$ for  $X\\in \\{1,2\\}$\n",
    "- $P(X=0, Y=0) = 1/2, P(X = 0, Y = 1) = 1/4, P(X = 1, Y = 0) = 1/8, P(X = 1, Y = 1) = 1/8$\n",
    "- $P(X=n)= 2^n, n\\in\\{1,2,3\\ldots,\\infty\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62e99d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1a I) The entropy is 1.0\n",
      "1a II) The entropy is 1.0\n",
      "1a III) The entropy is 1.75\n",
      "1a IV) Then entropy is 2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#1a I\n",
    "p = 1/2\n",
    "H = -p*np.log2(p)-(1-p)*np.log2(1-p)\n",
    "print(\"1a I) The entropy is\", H)\n",
    "\n",
    "# 1a II\n",
    "p = np.array([1/2, 1/2]) #Ignore 0 because it is 0 chance\n",
    "H = np.sum(-p*np.log2(p))\n",
    "print(\"1a II) The entropy is\", H)\n",
    "\n",
    "#1a III\n",
    "p = np.array([1/2,1/4,1/8,1/8])\n",
    "H = np.sum(-p*np.log2(p))\n",
    "print(\"1a III) The entropy is\", H)\n",
    "\n",
    "#1a IV\n",
    "n = 1\n",
    "run = True\n",
    "ssum = 0\n",
    "while run:\n",
    "    verdi = -2**(-n)*np.log2(2**(-n))\n",
    "    ssum = ssum + verdi\n",
    "    n+=1\n",
    "    if abs(verdi) <= 0.000001:\n",
    "        run = False\n",
    "print(\"1a IV) Then entropy is\", round(ssum,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53a4b5c",
   "metadata": {},
   "source": [
    "## 1b)\n",
    "Let $X,Y \\in \\{0,1\\}$. Let $P(Y=0 = 1/4, P(X=0|Y=0) = 1/2$ and $P(X=1|Y=1)=1/2$. Compute  $H(X), H(Y), H(X,Y), H(X|Y), H(Y|X)$ and $I(X;Y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29338930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(X) =  1.0\n",
      "H(Y) =  0.8112781244591328\n",
      "H(X,Y) =  1.811278124459133\n",
      "H(X|Y) =  1.0\n",
      "H(Y|X) =  0.8112781244591329\n",
      "I(X;Y) =  0.0\n"
     ]
    }
   ],
   "source": [
    "P_Y = np.array([1/4, 1-1/4])\n",
    "P_XgivenY = np.array([[1/2,1/2], # Rows: X=x, Columns Y=y\n",
    "                        [1/2,1/2]])\n",
    "P_X = P_XgivenY.dot(P_Y)\n",
    "\n",
    "P_XandY = list()\n",
    "P_XandY_log = list()\n",
    "P_YgivenX = np.empty(shape=(2,2))\n",
    "I_log = list()\n",
    "\n",
    "for i in range(0,2):\n",
    "    for j in range(0,2):\n",
    "        P_XandY.append(P_X[i]*P_Y[j])\n",
    "        P_XandY_log.append(P_X[j]*P_Y[i])\n",
    "        temp = P_XgivenY[i,j]*P_Y[j]/P_X[i]\n",
    "        P_YgivenX[i,j] = temp\n",
    "        I_log.append(P_X[i]*P_Y[j]/(P_X[i]*P_Y[j]))\n",
    "        \n",
    "HX = -np.sum(P_X*np.log2(P_X))\n",
    "HY = -np.sum(P_Y*np.log2(P_Y))\n",
    "HXandY = -np.sum(np.array(P_XandY)*np.log2(np.array(P_XandY)))\n",
    "HXgivenY = -np.sum(np.array(P_XandY)*np.log2(P_XgivenY.flatten()))\n",
    "HYgivenX = -np.sum(np.array(P_XandY)*np.log2(P_YgivenX.flatten()))\n",
    "IXY = HX - HXgivenY\n",
    "\n",
    "print(\"H(X) = \",HX)\n",
    "print(\"H(Y) = \",HY)\n",
    "print(\"H(X,Y) = \",HXandY)\n",
    "print(\"H(X|Y) = \",HXgivenY)\n",
    "print(\"H(Y|X) = \",HYgivenX)\n",
    "print(\"I(X;Y) = \",IXY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd1fbf9",
   "metadata": {},
   "source": [
    "## 1c) \n",
    "(Hard) A Markov chain is a sequence $X1, X2, X3,\\ldots$ of random variables that has the following property:\n",
    "\n",
    "\\begin{align}\n",
    "P(Xn|X_{n-1}, X_{n-2}, X_{n-3}, \\ldots) = P(Xn|Xn−1)\n",
    "\\end{align}\n",
    "for all *n*. We can describe a Markov chain completely by its initial probabilities $P(X1)$ and\n",
    "transition probabilities $P(Xn|Xn−1)$. Let $X ∈ \\{0, 1\\}$ and\n",
    "\n",
    "\\begin{align}\n",
    "P(X = 0) = 1/2 \\\\\n",
    "P(X_{n} = 0|X_{n-2} = 0) = 7/8 \\\\\n",
    "P(X_{n} = 1|X_{n-1} = 1) = 7/8\n",
    "\\end{align}\n",
    "Find the steady state distribution for X, which is the probability $P(Xn = x)$ for any *n*. Then\n",
    "compute the entropy rate of the sequence $X1, X2, X3, \\ldots, Xn$ as *n* goes to infinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "515b00bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stationary_distribution(transition_matrix):\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(transition_matrix.T)\n",
    "    stationary_distribution = eigenvectors[:, np.isclose(eigenvalues, 1)].flatten()\n",
    "    return stationary_distribution/np.sum(stationary_distribution)\n",
    "\n",
    "def get_entropy_rate(transition_matrix):\n",
    "    steady_state = get_stationary_distribution(transition_matrix)\n",
    "    entropy_rate = 0\n",
    "    for i in range(0, len(steady_state)):\n",
    "        for j in range(0, len(steady_state)):\n",
    "            entropy_rate += steady_state[i]*transition_matrix[i,j]*np.log2(transition_matrix[i,j]) if transition_matrix[i,j] != 0 else 0\n",
    "    return -entropy_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de8f7f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stady state distribution is:  [0.5 0.5]\n",
      "entropy rate is:  0.5435644431995964\n"
     ]
    }
   ],
   "source": [
    "transition_matrix = np.array([[7/8,1/8],[1/8,7/8]])\n",
    "steady_state_distribution = get_stationary_distribution(transition_matrix)\n",
    "\n",
    "print(\"stady state distribution is: \",steady_state_distribution)\n",
    "print(\"entropy rate is: \", get_entropy_rate(transition_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0884ab7d",
   "metadata": {},
   "source": [
    "The next problems involve numerical exercises using Python. There are three data files\n",
    "named _data[123].txt_ that shall be used for the problems below. They are all text files with a\n",
    "single 0 or 1 per line. You can easily load the files with _numpy.loadtxt_ (see NumPy manual)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df026418",
   "metadata": {},
   "source": [
    "## 1d) \n",
    "Estimate the entropy for each of the three files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a47746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(data, max_val):\n",
    "    symbols = dict()\n",
    "    prob = list()\n",
    "    for i in range(0, max_val+1):\n",
    "        symbols[bin(i)[2:].zfill(len(data[0]))] = 0\n",
    "    for i in data:\n",
    "        symbols[i] += 1\n",
    "    for key in symbols:\n",
    "        prob.append(symbols[key]/len(data))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb7ce63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(data1) =  0.8113854260218433\n",
      "H(data2) =  0.8112781244591328\n",
      "H(data3) =  0.8107562272490967\n"
     ]
    }
   ],
   "source": [
    "data1 = np.loadtxt(\"data1.txt\")\n",
    "data1 = [str(int(i)) for i in data1]\n",
    "data2 = np.loadtxt(\"data2.txt\")\n",
    "data2 = [str(int(i)) for i in data2]\n",
    "data3 = np.loadtxt(\"data3.txt\")\n",
    "data3 = [str(int(i)) for i in data3]\n",
    "\n",
    "datas = [data1, data2, data3]\n",
    "P_datas = list()\n",
    "for i in datas:\n",
    "    prob = [i for i in get_prob(i,1) if i != 0] \n",
    "    P_datas.append(prob)\n",
    "\n",
    "Hdatas = list()\n",
    "for i in P_datas:\n",
    "    Hdatas.append(-np.sum(i*np.log2(i)))\n",
    "\n",
    "print(\"H(data1) = \",Hdatas[0])\n",
    "print(\"H(data2) = \",Hdatas[1])\n",
    "print(\"H(data3) = \",Hdatas[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a959614",
   "metadata": {},
   "source": [
    "## 1e) \n",
    "Estimate the entropy for each of the three files when considering two, three and four bits\n",
    "at the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ef3f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_bits(file, num):\n",
    "    new_array = list()\n",
    "    for i in range(0, len(file), num):\n",
    "        try:\n",
    "            word = \"\"\n",
    "            for j in range(0, num):\n",
    "                word += str(int(file[i+j]))\n",
    "            new_array.append(word)\n",
    "        except:\n",
    "            pass\n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6b1348a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(data1_2bit) =  0.8113851861498752\n",
      "H(data2_2bit) =  0.7499999027552386\n",
      "H(data3_2bit) =  0.5672105369346512\n",
      "\n",
      "H(data1_3bit) =  0.8113842324124635\n",
      "H(data2_3bit) =  0.6666665437480547\n",
      "H(data3_3bit) =  0.486741388652076\n",
      "\n",
      "H(data1_4bit) =  0.8113824233775777\n",
      "H(data2_4bit) =  0.24999990275523865\n",
      "H(data3_4bit) =  0.4454849456126865\n"
     ]
    }
   ],
   "source": [
    "datas_2bit = list()\n",
    "for i in datas:\n",
    "    datas_2bit.append(merge_bits(i,2))\n",
    "\n",
    "P_datas_2bit = list()\n",
    "for i in datas_2bit:\n",
    "    prob = [i for i in get_prob(i,3) if i != 0] \n",
    "    P_datas_2bit.append(prob)\n",
    "\n",
    "Hdatas_2bit = list()\n",
    "for i in P_datas_2bit:\n",
    "    Hdatas_2bit.append(-np.sum(i*np.log2(i))/2)\n",
    "\n",
    "print(\"H(data1_2bit) = \",Hdatas_2bit[0])\n",
    "print(\"H(data2_2bit) = \",Hdatas_2bit[1])\n",
    "print(\"H(data3_2bit) = \",Hdatas_2bit[2])\n",
    "##########################################################\n",
    "datas_3bit = list()\n",
    "for i in datas:\n",
    "    datas_3bit.append(merge_bits(i,3))\n",
    "\n",
    "P_datas_3bit = list()\n",
    "for i in datas_3bit:\n",
    "    prob = [i for i in get_prob(i,7) if i != 0] \n",
    "    P_datas_3bit.append(prob)\n",
    "\n",
    "Hdatas_3bit = list()\n",
    "for i in P_datas_3bit:\n",
    "    Hdatas_3bit.append(-np.sum(i*np.log2(i))/3)\n",
    "\n",
    "print(\"\\nH(data1_3bit) = \",Hdatas_3bit[0])\n",
    "print(\"H(data2_3bit) = \",Hdatas_3bit[1])\n",
    "print(\"H(data3_3bit) = \",Hdatas_3bit[2])\n",
    "##########################################################\n",
    "datas_4bit = list()\n",
    "for i in datas:\n",
    "    datas_4bit.append(merge_bits(i,4))\n",
    "\n",
    "P_datas_4bit = list()\n",
    "for i in datas_4bit:\n",
    "    prob = [i for i in get_prob(i,15) if i != 0] \n",
    "    P_datas_4bit.append(prob)\n",
    "\n",
    "Hdatas_4bit = list()\n",
    "for i in P_datas_4bit:\n",
    "    Hdatas_4bit.append(-np.sum(i*np.log2(i))/4)\n",
    "\n",
    "print(\"\\nH(data1_4bit) = \",Hdatas_4bit[0])\n",
    "print(\"H(data2_4bit) = \",Hdatas_4bit[1])\n",
    "print(\"H(data3_4bit) = \",Hdatas_4bit[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a79b4b0",
   "metadata": {},
   "source": [
    "## 1f)\n",
    "Attempt to estimate the entropy rate for each of the three files. Note that you will need\n",
    "to approximate the rate by some finite symbols length n which is up to you to determine\n",
    "(Note: The choice of n is important as increasing n arbitrarily will lead to a entropy rate\n",
    "equal to zero. Why?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61d754b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 bit entropy rate of data1 =  0.8113808989188748\n",
      "5 bit entropy rate of data2 =  0.5561170065840938\n",
      "5 bit entropy rate of data3 =  0.42135525722506956\n",
      "\n",
      "When increasing n arbitrariliy the entroyp rate goes to zero, becuase the number of possible states increases exponentially, but most of the states will have a probability of zero and will not contribute to the entropy, then you divide by n and the entropy rate goes to zero.\n",
      "6 bit entropy rate of data1 =  0.8113767725492664\n",
      "6 bit entropy rate of data2 =  0.41666626572020776\n",
      "6 bit entropy rate of data3 =  0.4051741490508978\n",
      "\n",
      "When increasing n arbitrariliy the entroyp rate goes to zero, becuase the number of possible states increases exponentially, but most of the states will have a probability of zero and will not contribute to the entropy, then you divide by n and the entropy rate goes to zero.\n",
      "7 bit entropy rate of data1 =  0.8113661111931163\n",
      "7 bit entropy rate of data2 =  0.4999611873858086\n",
      "7 bit entropy rate of data3 =  0.3936078807882429\n",
      "\n",
      "When increasing n arbitrariliy the entroyp rate goes to zero, becuase the number of possible states increases exponentially, but most of the states will have a probability of zero and will not contribute to the entropy, then you divide by n and the entropy rate goes to zero.\n",
      "8 bit entropy rate of data1 =  0.8113358399193957\n",
      "8 bit entropy rate of data2 =  0.2499997419409887\n",
      "8 bit entropy rate of data3 =  0.3847562692808571\n",
      "\n",
      "When increasing n arbitrariliy the entroyp rate goes to zero, becuase the number of possible states increases exponentially, but most of the states will have a probability of zero and will not contribute to the entropy, then you divide by n and the entropy rate goes to zero.\n",
      "9 bit entropy rate of data1 =  0.8112911662381618\n",
      "9 bit entropy rate of data2 =  0.46017918321326895\n",
      "9 bit entropy rate of data3 =  0.37824723434770313\n",
      "\n",
      "When increasing n arbitrariliy the entroyp rate goes to zero, becuase the number of possible states increases exponentially, but most of the states will have a probability of zero and will not contribute to the entropy, then you divide by n and the entropy rate goes to zero.\n",
      "10 bit entropy rate of data1 =  0.8112080039785136\n",
      "10 bit entropy rate of data2 =  0.34999823857974743\n",
      "10 bit entropy rate of data3 =  0.3724755547990849\n",
      "\n",
      "When increasing n arbitrariliy the entroyp rate goes to zero, becuase the number of possible states increases exponentially, but most of the states will have a probability of zero and will not contribute to the entropy, then you divide by n and the entropy rate goes to zero.\n",
      "11 bit entropy rate of data1 =  0.8110469980316246\n",
      "11 bit entropy rate of data2 =  0.4318801289494074\n",
      "11 bit entropy rate of data3 =  0.36791809770152395\n",
      "\n",
      "When increasing n arbitrariliy the entroyp rate goes to zero, becuase the number of possible states increases exponentially, but most of the states will have a probability of zero and will not contribute to the entropy, then you divide by n and the entropy rate goes to zero.\n",
      "12 bit entropy rate of data1 =  0.810658340811222\n",
      "12 bit entropy rate of data2 =  0.24999885119728496\n",
      "12 bit entropy rate of data3 =  0.36423835381489017\n",
      "\n",
      "When increasing n arbitrariliy the entroyp rate goes to zero, becuase the number of possible states increases exponentially, but most of the states will have a probability of zero and will not contribute to the entropy, then you divide by n and the entropy rate goes to zero.\n",
      "13 bit entropy rate of data1 =  0.8099524702189114\n",
      "13 bit entropy rate of data2 =  0.40922377531563103\n",
      "13 bit entropy rate of data3 =  0.3607817930755242\n",
      "\n",
      "When increasing n arbitrariliy the entroyp rate goes to zero, becuase the number of possible states increases exponentially, but most of the states will have a probability of zero and will not contribute to the entropy, then you divide by n and the entropy rate goes to zero.\n",
      "14 bit entropy rate of data1 =  0.8086853928027217\n",
      "14 bit entropy rate of data2 =  0.32142587605261436\n",
      "14 bit entropy rate of data3 =  0.35806103256454996\n",
      "\n",
      "When increasing n arbitrariliy the entroyp rate goes to zero, becuase the number of possible states increases exponentially, but most of the states will have a probability of zero and will not contribute to the entropy, then you divide by n and the entropy rate goes to zero.\n",
      "15 bit entropy rate of data1 =  0.806446561364948\n",
      "15 bit entropy rate of data2 =  0.39163632862418585\n",
      "15 bit entropy rate of data3 =  0.3555308662804556\n",
      "\n",
      "When increasing n arbitrariliy the entroyp rate goes to zero, becuase the number of possible states increases exponentially, but most of the states will have a probability of zero and will not contribute to the entropy, then you divide by n and the entropy rate goes to zero.\n",
      "16 bit entropy rate of data1 =  0.8027591551312054\n",
      "16 bit entropy rate of data2 =  0.24999832989011803\n",
      "16 bit entropy rate of data3 =  0.35294000326709524\n",
      "\n",
      "When increasing n arbitrariliy the entroyp rate goes to zero, becuase the number of possible states increases exponentially, but most of the states will have a probability of zero and will not contribute to the entropy, then you divide by n and the entropy rate goes to zero.\n",
      "17 bit entropy rate of data1 =  0.7973335730742818\n",
      "17 bit entropy rate of data2 =  0.37708271879148436\n",
      "17 bit entropy rate of data3 =  0.3505529913581152\n",
      "\n",
      "When increasing n arbitrariliy the entroyp rate goes to zero, becuase the number of possible states increases exponentially, but most of the states will have a probability of zero and will not contribute to the entropy, then you divide by n and the entropy rate goes to zero.\n",
      "18 bit entropy rate of data1 =  0.7896678128425854\n",
      "18 bit entropy rate of data2 =  0.3055464539636149\n",
      "18 bit entropy rate of data3 =  0.34868168371237795\n",
      "\n",
      "When increasing n arbitrariliy the entroyp rate goes to zero, becuase the number of possible states increases exponentially, but most of the states will have a probability of zero and will not contribute to the entropy, then you divide by n and the entropy rate goes to zero.\n",
      "19 bit entropy rate of data1 =  0.7794503437358746\n",
      "19 bit entropy rate of data2 =  0.365156388409704\n",
      "19 bit entropy rate of data3 =  0.346773037914747\n",
      "\n",
      "When increasing n arbitrariliy the entroyp rate goes to zero, becuase the number of possible states increases exponentially, but most of the states will have a probability of zero and will not contribute to the entropy, then you divide by n and the entropy rate goes to zero.\n",
      "20 bit entropy rate of data1 =  0.7665632039974468\n",
      "20 bit entropy rate of data2 =  0.24999482729609873\n",
      "20 bit entropy rate of data3 =  0.34462187850422554\n",
      "\n",
      "When increasing n arbitrariliy the entroyp rate goes to zero, becuase the number of possible states increases exponentially, but most of the states will have a probability of zero and will not contribute to the entropy, then you divide by n and the entropy rate goes to zero.\n"
     ]
    }
   ],
   "source": [
    "N = list()\n",
    "\n",
    "for i in range(5, 21):\n",
    "    N.append(i)\n",
    "\n",
    "for n in N:\n",
    "    datas_Nbit = list()\n",
    "    for i in datas:\n",
    "        datas_Nbit.append(merge_bits(i,n))\n",
    "\n",
    "    P_datas_Nbit = list()\n",
    "    for i in datas_Nbit:\n",
    "        prob = [i for i in get_prob(i,2**n-1) if i != 0] \n",
    "        P_datas_Nbit.append(prob)\n",
    "\n",
    "    Hdatas_Nbit = list()\n",
    "    for i in P_datas_Nbit:\n",
    "        Hdatas_Nbit.append(-np.sum(i*np.log2(i)))\n",
    "\n",
    "    print(f\"{n} bit entropy rate of data1 = \",Hdatas_Nbit[0]/n)\n",
    "    print(f\"{n} bit entropy rate of data2 = \",Hdatas_Nbit[1]/n)\n",
    "    print(f\"{n} bit entropy rate of data3 = \",Hdatas_Nbit[2]/n)\n",
    "    print(\"\\nWhen increasing n arbitrariliy the entroyp rate goes to zero, becuase the number of possible states increases exponentially, but most of the states will have a probability of zero and will not contribute to the entropy, then you divide by n and the entropy rate goes to zero.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c31eec",
   "metadata": {},
   "source": [
    "## 1g)\n",
    "Try and model each of the three data files as a Markov chain. Estimate the transition and\n",
    "steady state parameters and compute the entropy. Compare with the entropy rate from\n",
    "the previous problem. Which problem is likely to come from a binary Markov chain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "435b9032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_matrix(data):\n",
    "    symbols = dict()\n",
    "    max_val = 2**len(data[0])\n",
    "    for i in range(0, max_val):\n",
    "        symbols[bin(i)[2:].zfill(len(data[0]))] = dict()\n",
    "        for j in range(0, max_val):\n",
    "            symbols[bin(i)[2:].zfill(len(data[0]))][bin(j)[2:].zfill(len(data[0]))] = 0\n",
    "    for i in range(0, len(data)-1):\n",
    "        symbols[data[i]][data[i+1]] += 1\n",
    "    transition_matrix = np.zeros((max_val,max_val))\n",
    "    row = 0\n",
    "    for i in symbols:\n",
    "        column = 0\n",
    "        count = sum(symbols[i].values())\n",
    "        for j in symbols[i]:\n",
    "            transition_matrix[row,column] = symbols[i][j]/count if count != 0 else 0\n",
    "            column += 1\n",
    "        row += 1\n",
    "    return transition_matrix     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d09a8e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition matrix of data1 =\n",
      " [[0.24975116 0.75024884]\n",
      " [0.25017335 0.74982665]]\n",
      "\n",
      "transition matrix of data2 =\n",
      " [[0.         1.        ]\n",
      " [0.33333344 0.66666656]]\n",
      "\n",
      "transition matrix of data3 =\n",
      " [[0.87448577 0.12551423]\n",
      " [0.04176471 0.95823529]]\n",
      "\n",
      "steady state of data1 = [0.25006777 0.74993223]\n",
      "\n",
      "steady state of data2 = [0.25000006 0.74999994]\n",
      "\n",
      "steady state of data3 = [0.24967104 0.75032896]\n",
      "\n",
      "entropy rate of data1 = 0.8113853918886793\n",
      "\n",
      "entropy rate of data2 = 0.6887219002790084\n",
      "\n",
      "entropy rate of data3 = 0.32389885074769115\n",
      "\n",
      "entropy rate of previous problem:\n",
      "entropy rate of data1 = 0.766563203997446820\n",
      "entropy rate of data2 = 0.2499948272960987320\n",
      "entropy rate of data3 = 0.34462187850422554\n",
      "\n",
      "\n",
      "wee see that the result for data1 and data3 are fairly close, but the result for data2 are very different, this could be due to sequence not being stationary\n"
     ]
    }
   ],
   "source": [
    "datas_transition_matrix = [get_transition_matrix(i) for i in datas]\n",
    "\n",
    "print(\"transition matrix of data1 =\\n\",datas_transition_matrix[0])\n",
    "print(\"\\ntransition matrix of data2 =\\n\",datas_transition_matrix[1])\n",
    "print(\"\\ntransition matrix of data3 =\\n\",datas_transition_matrix[2])\n",
    "\n",
    "print(\"\\nsteady state of data1 =\",get_stationary_distribution(datas_transition_matrix[0]))\n",
    "print(\"\\nsteady state of data2 =\",get_stationary_distribution(datas_transition_matrix[1]))\n",
    "print(\"\\nsteady state of data3 =\",get_stationary_distribution(datas_transition_matrix[2]))\n",
    "\n",
    "print(\"\\nentropy rate of data1 =\",get_entropy_rate(datas_transition_matrix[0]))\n",
    "print(\"\\nentropy rate of data2 =\",get_entropy_rate(datas_transition_matrix[1]))\n",
    "print(\"\\nentropy rate of data3 =\",get_entropy_rate(datas_transition_matrix[2])) \n",
    "\n",
    "print(\"\\nentropy rate of previous problem:\\nentropy rate of data1 = 0.766563203997446820\\nentropy rate of data2 = 0.2499948272960987320\\nentropy rate of data3 = 0.34462187850422554\\n\\n\")\n",
    "print(\"wee see that the result for data1 and data3 are fairly close, but the result for data2 are very different, this could be due to sequence not being stationary\")\n",
    "\n",
    "print(\"\\n\\nI can't find much information about the issues exclusive to a binary markov chain. my guess would be that that they are limited in representing real world systems due to only having two states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d439f421",
   "metadata": {},
   "source": [
    "\n",
    "A run length code can be defined as follows. Given a bit sequence\n",
    "\\begin{align}\n",
    "\\\\\n",
    "S = 111001111100111110000\\\\\n",
    "\\end{align}\n",
    "\n",
    "we can code it as\n",
    "\n",
    "\\begin{align}\n",
    "\\\\\n",
    "C = 0325254\\\\\n",
    "\\end{align}\n",
    "\n",
    "which can be interpreted as “zero 0s, three 1s, 2 0s, 5 1s, ...” etc. We have assumed that\n",
    "we always start counting the zeros (here there are zero of them). We also need to determine\n",
    "what the maximum length we can represent is, and how we will represent sequences longer\n",
    "than this. Here we use the convention that if a sequence of eg. zeros are larger than some\n",
    "maximum N, we write the code as N0M where M is the remainder of the symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28bddbd",
   "metadata": {},
   "source": [
    "## 1h)\n",
    "Implement a run-length code in Python and code the file _data3.txt_, using\n",
    "\\begin{align}\n",
    "\\\\\n",
    "N ∈ \\{7, 15, 31, 63, 127\\}\\\\\n",
    "\\end{align}\n",
    "\n",
    "What is the coding gain for those three cases? How does it compare with the entropy\n",
    "rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7f61be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_length_encoder(data, n):\n",
    "    bit_length = int(np.log2(n+1))\n",
    "    bol = False\n",
    "    encoded = \"\"\n",
    "    count = 0\n",
    "    i = 0\n",
    "    while i < len(data):\n",
    "        if count==n:\n",
    "            encoded += bin(count)[2:].zfill(bit_length)\n",
    "            count = 0\n",
    "        elif data[i] != str(int(bol)):\n",
    "            encoded += bin(count)[2:].zfill(bit_length)\n",
    "            count = 0\n",
    "            bol = not bol\n",
    "        else:\n",
    "            count += 1\n",
    "            i += 1\n",
    "    encoded += bin(count)[2:].zfill(bit_length)\n",
    "    if count==n:\n",
    "        encoded += bin(0)[2:].zfill(bit_length)\n",
    "    return encoded\n",
    "\n",
    "def run_length_decoder(data, n):\n",
    "    bit_length = int(np.log2(n+1))\n",
    "    bol = False\n",
    "    decoded = \"\"\n",
    "    for i in range(0, len(data), bit_length):\n",
    "        decimal = int(data[i:i+bit_length],2)\n",
    "        decoded += str(int(bol))*decimal\n",
    "        if decimal != n:\n",
    "            bol = not bol\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01990239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_length_encoder_right_version(data, n):\n",
    "    bit_length = int(np.log2(n+1))\n",
    "    bol = False\n",
    "    encoded = \"\"\n",
    "    count = 0\n",
    "    i = 0\n",
    "    while i < len(data):\n",
    "        if count==n:\n",
    "            encoded += bin(count)[2:].zfill(bit_length)\n",
    "            count = 0\n",
    "            bol = not bol\n",
    "        elif data[i] != str(int(bol)):\n",
    "            encoded += bin(count)[2:].zfill(bit_length)\n",
    "            count = 0\n",
    "            bol = not bol\n",
    "        else:\n",
    "            count += 1\n",
    "            i += 1\n",
    "    encoded += bin(count)[2:].zfill(bit_length)\n",
    "    return encoded\n",
    "\n",
    "def run_length_decoder_right_version(data, n):\n",
    "    bit_length = int(np.log2(n+1))\n",
    "    bol = False\n",
    "    decoded = \"\"\n",
    "    for i in range(0, len(data), bit_length):\n",
    "        decimal = int(data[i:i+bit_length],2)\n",
    "        decoded += str(int(bol))*decimal\n",
    "        bol = not bol\n",
    "    return decoded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a003fb7",
   "metadata": {},
   "source": [
    "## 1i)\n",
    "The file _data4.bin_ contains a version of the binary data from _data3.txt_ where the bits\n",
    "have been packed into bytes and stored as a binary file. Use any compression program\n",
    "of you choice (Zip, GZip, BZip2, 7Zip etc.) to compress the file and compare the results\n",
    "with the entropy rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c309da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we initially misunderstood how the run length code should be implemented and thought that counts larger than n should be split into n and the rest immeditaletly after,\n",
      "but this is not the case, the count should be n and then the next bit should be the opposite of the current bit, so we implemented the right version of the encoder,\n",
      "but this gave a worse result, so we decided to keep the wrong version of the encoder, but we kept the right version of the encoder as well\n",
      "\n",
      "\n",
      "N = 7 :\n",
      "encoded data3 size =  2260011\n",
      "encoded data3 gain =  0.5388286113739014\n",
      "\n",
      "N = 15 :\n",
      "encoded data3 size =  1755900\n",
      "encoded data3 gain =  0.4186391830444336\n",
      "\n",
      "N = 31 :\n",
      "encoded data3 size =  1576290\n",
      "encoded data3 gain =  0.37581682205200195\n",
      "\n",
      "N = 63 :\n",
      "encoded data3 size =  1637850\n",
      "encoded data3 gain =  0.39049386978149414\n",
      "\n",
      "N = 127 :\n",
      "encoded data3 size =  1844794\n",
      "encoded data3 gain =  0.4398331642150879\n",
      "no error\n",
      "\n",
      "By using 7Zip, the file got compressed down to 196 kB = 1568000 bits\n",
      "That is a compression ratio of 196/516 KB = 0.38\n",
      "The best compression ratio was acheived by using N = 31\n",
      "Our code made it only 0.5% bigger than what 7Zip did\n"
     ]
    }
   ],
   "source": [
    "print(\"we initially misunderstood how the run length code should be implemented and thought that counts larger than n should be split into n and the rest immeditaletly after,\") \n",
    "print(\"but this is not the case, the count should be n and then the next bit should be the opposite of the current bit, so we implemented the right version of the encoder,\")\n",
    "print(\"but this gave a worse result, so we decided to keep the wrong version of the encoder, but we kept the right version of the encoder as well\\n\")\n",
    "N = [7, 15, 31, 63, 127]\n",
    "\n",
    "for i in N:\n",
    "    encoded_data3 = run_length_encoder(datas[2], i)\n",
    "    print(f\"\\nN = {i} :\")\n",
    "    print(f\"encoded data3 size = \",len(encoded_data3))\n",
    "    print(f\"encoded data3 gain = \",len(encoded_data3)/len(datas[2]))\n",
    "\n",
    "encoded_data3 = run_length_encoder(datas[2], 63)\n",
    "\n",
    "decoded_data3 = run_length_decoder(encoded_data3, 63)\n",
    "\n",
    "for i in range(len(datas[2])):\n",
    "    if datas[2][i] != decoded_data3[i]:\n",
    "        print(\"error\")\n",
    "        break\n",
    "else:\n",
    "    print(\"no error\")\n",
    "    \n",
    "print(\"\\nBy using 7Zip, the file got compressed down to 196 kB = 1568000 bits\")\n",
    "print(\"That is a compression ratio of 196/516 KB = 0.38\")\n",
    "\n",
    "print(\"The best compression ratio was acheived by using N = 31\")\n",
    "print(\"Our code made it only 0.5% bigger than what 7Zip did\")\n",
    "print(\"our best compression here was rate was roughtly 0.376 while the entropy rate for data3 was 0.324\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "413d8d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N = 7 :\n",
      "encoded data3 size =  3559680\n",
      "encoded data3 gain fast =  0.84869384765625\n",
      "\n",
      "N = 15 :\n",
      "encoded data3 size =  2386040\n",
      "encoded data3 gain fast =  0.5688762664794922\n",
      "\n",
      "N = 31 :\n",
      "encoded data3 size =  1813040\n",
      "encoded data3 gain fast =  0.4322624206542969\n",
      "\n",
      "N = 63 :\n",
      "encoded data3 size =  1693512\n",
      "encoded data3 gain fast =  0.4037647247314453\n",
      "\n",
      "N = 127 :\n",
      "encoded data3 size =  1849106\n",
      "encoded data3 gain fast =  0.44086122512817383\n",
      "no error\n",
      "\n",
      "By using 7Zip, the file got compressed down to 196 kB = 1568000 bits\n",
      "That is a compression ratio of 196/516 KB = 0.38\n",
      "The best compression ratio was acheived by using N = 31\n",
      "Our code made it only 0.5% bigger than what 7Zip did\n"
     ]
    }
   ],
   "source": [
    "N = [7, 15, 31, 63, 127]\n",
    "\n",
    "for i in N:\n",
    "    encoded_data3_right = run_length_encoder_right_version(datas[2], i)\n",
    "    print(f\"\\nN = {i} :\")\n",
    "    print(f\"encoded data3 size = \",len(encoded_data3_right))\n",
    "    print(f\"encoded data3 gain = \",len(encoded_data3_right)/len(datas[2]))\n",
    "\n",
    "decoded_data3_right = run_length_decoder_right_version(encoded_data3_right, 127)\n",
    "\n",
    "for i in range(len(datas[2])):\n",
    "    if datas[2][i] != decoded_data3_right[i]:\n",
    "        print(\"error\")\n",
    "        break\n",
    "else:\n",
    "    print(\"no error\")\n",
    "\n",
    "    \n",
    "print(\"\\nBy using 7Zip, the file got compressed down to 196 kB = 1568000 bits\")\n",
    "print(\"That is a compression ratio of 196/516 KB = 0.38\")\n",
    "\n",
    "print(\"The best compression ratio was acheived by using N = 63\")\n",
    "print(\"Our code made it only 0.5% bigger than what 7Zip did\")\n",
    "print(\"the lowest size we got with the right verion was roughly 1.69Mb, which is larger than the 1.576Mb we got with the wrong version\")\n",
    "print(\"our best compression rate here was roughtly 0.4 while the entropy rate for data3 was 0.324\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb4a53e",
   "metadata": {},
   "source": [
    "## Problem 2 Deep neural networks\n",
    "This problem is designed to be very open and is aiming to be similar to the process of\n",
    "training a deep neural network from scratch.\\\n",
    "\\\n",
    "You will find the file _problem2_template.py_ attached. This file contains a bare bones script\n",
    "for training a very simple classifier on the **CIFAR 10** datasets (_https://www.cs.toronto.edu/\n",
    "~kriz/cifar.html_). This is a classification task that has ten classes - **’plane’, ’car’, ’bird’, ’cat’,\n",
    "’deer’, ’dog’, ’frog’, ’horse’, ’ship’ and ’truck’**.\\\n",
    "We want to you create a DNN classifier and use it to do image retrieval on the test set. For\n",
    "example, if you decide to retrieve images of dogs, you should measure the accuracy, precision\n",
    "and recall for this task. We also want you to use the AUPRC (area under the PR curve) to\n",
    "rank the different systems you make.\\\n",
    "In your task you are free to add new layers, increase the number of nodes per layer, add\n",
    "dropout and change the optimization algorithm if so be. The dataset is small, but the training\n",
    "will take some time, so you will need to make sure you pick the lowest hanging fruits first.\n",
    "The answer to this problem should of course be the precision-recall curve for the retrival\n",
    "problem, but also a description of how you improved the original neural network with the\n",
    "relevant results in tables and graphs.\\\n",
    "\\\n",
    "As a starting point you should to the following:\n",
    "1. Run the script and make sure it works. You will need the two Python modules *torch*\n",
    "and *torchvision*. Make a note of the time it takes to finish the training, and adjust the\n",
    "number of epochs to make your turn-around between experiments quick enough.\n",
    "2. Divide the training set into a training and validation set. You can do this manually or\n",
    "using tools from *scikit-learn*.\n",
    "3. Add layers to the neural network and observe the performance on the validation set.\n",
    "4. Use *scikit-learn* to compute PR curves for the retrieval problem and compare the\n",
    "results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f17acc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1\n",
      "Training Loss: 2.19\n",
      "Validation Loss: 1.89\n",
      "Validation Loss Decreased (inf--->1.89)\n",
      "Saving The Model\n",
      "\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "Training Loss: 1.66\n",
      "Validation Loss: 1.48\n",
      "Validation Loss Decreased (1.9--->1.48)\n",
      "Saving The Model\n",
      "\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "Training Loss: 1.42\n",
      "Validation Loss: 1.34\n",
      "Validation Loss Decreased (1.5--->1.34)\n",
      "Saving The Model\n",
      "\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "Training Loss: 1.29\n",
      "Validation Loss: 1.34\n",
      "Validation Loss Decreased (1.3--->1.34)\n",
      "Saving The Model\n",
      "\n",
      "\n",
      "\n",
      "Epoch 5\n",
      "Training Loss: 1.19\n",
      "Validation Loss: 1.2\n",
      "Validation Loss Decreased (1.3--->1.2)\n",
      "Saving The Model\n",
      "\n",
      "\n",
      "\n",
      "Epoch 6\n",
      "Training Loss: 1.1\n",
      "Validation Loss: 1.17\n",
      "Validation Loss Decreased (1.2--->1.17)\n",
      "Saving The Model\n",
      "\n",
      "\n",
      "\n",
      "Epoch 7\n",
      "Training Loss: 1.02\n",
      "Validation Loss: 1.2\n",
      "\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 58 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                     (0.5, 0.5, 0.5))])\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "kwargs = {} if device=='cpu' else {'num_workers': 1, 'pin_memory': True}\n",
    "\n",
    "batch_size=8\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "# Define how much of the dataset should be validation set\n",
    "val_size = 0.25\n",
    "\n",
    "#Split dataset to create training- and validation set\n",
    "train_idx, val_idx = train_test_split(list(range(len(dataset))),test_size = val_size)\n",
    "\n",
    "trainset = Subset(dataset, train_idx) \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, **kwargs)\n",
    "valset   = Subset(dataset, val_idx) \n",
    "validloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                          shuffle=True, **kwargs)\n",
    "\n",
    "# Import testset\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, **kwargs)\n",
    "\n",
    "# Define classes\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(400, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 400)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()    # Define network\n",
    "net.to(device) # Use CPU or GPU\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "##########################################################################################################################  \n",
    "\n",
    "epochs = 7\n",
    "min_valid_loss = np.inf\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    training_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        # inputs, labels = data\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        training_loss += loss.item()\n",
    "        \n",
    "    valid_loss = 0.0\n",
    "    net.eval()     # Optional when not using Model Specific layer\n",
    "    for data, labels in validloader:\n",
    "        # Transfer Data to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "        # Forward Pass\n",
    "        target = net(data)\n",
    "        # Find the Loss\n",
    "        loss = criterion(target,labels)\n",
    "        # Calculate Loss\n",
    "        valid_loss += loss.item()\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    print(f\"Training Loss: {(training_loss / len(trainloader)):.3}\")\n",
    "    print(f\"Validation Loss: {(valid_loss / len(validloader)):.3}\")\n",
    "     \n",
    "    if min_valid_loss > valid_loss:\n",
    "        print(f\"Validation Loss Decreased ({(min_valid_loss / len(validloader)):.2}--->{(valid_loss / len(validloader)):.3})\")\n",
    "        print(\"Saving The Model\")\n",
    "        min_valid_loss = valid_loss\n",
    "         \n",
    "        # Saving State Dict\n",
    "        #torch.save(net.state_dict(), 'saved_model.pth')\n",
    "\n",
    "    \n",
    "##########################################################################################################################    \n",
    "       \n",
    "    \n",
    "print(\"\\nFinished Training\")\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        # images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%'\n",
    "      % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd0d1a4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m prob \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m10\u001b[39m\n\u001b[0;32m     10\u001b[0m index \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 11\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39;49m(outputs\u001b[39m.\u001b[39;49mcpu()):\n\u001b[0;32m     12\u001b[0m     \u001b[39mif\u001b[39;00m outputs[i] \u001b[39m>\u001b[39m prob:\n\u001b[0;32m     13\u001b[0m         prob \u001b[39m=\u001b[39m outputs[i]\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "#retrieve dogs\n",
    "\n",
    "#outputs in the folling code returns an array containing the probability of each class. if the class for dog has the highest probability then we want to retrieve that image\n",
    "#but i have no clue which index belongs to dog, cause i can't read the testloader file. there lets say index \"2\" is dog\n",
    "\n",
    "for data in testloader:\n",
    "    images, labels = data[0].to(device), data[1].to(device)\n",
    "    outputs = net(images)\n",
    "    prob = -10\n",
    "    index = 0\n",
    "    for i in range(outputs.numpy()):\n",
    "        if outputs[i] > prob:\n",
    "            prob = outputs[i]\n",
    "            index = i\n",
    "    if index == 2:\n",
    "        print(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e26198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Test the model\n",
    "print(\"Testing the model\")\n",
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(images)\n",
    "        predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "# Compute PR curve\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "labels = testset.targets\n",
    "precisions, recalls, thresholds = precision_recall_curve(labels, predictions[:, 1], pos_label=1)\n",
    "\n",
    "# Plot PR curve\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(recalls, precisions)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e49dd89b",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "The first run took 90 seconds and had an accuracy of 60%\\\n",
    "\n",
    "After trying out different combinations of epochs, batch size, number of layers and different dropout functions, we only managed to get 61% accuracy. This takes 120 seconds to run."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "835a0a66f5e0e0b5327b638cecd9416a1b95bb1147974c9b466cb34bc814813a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
