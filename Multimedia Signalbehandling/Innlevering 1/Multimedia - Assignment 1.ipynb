{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c20e6045",
   "metadata": {},
   "source": [
    "# TTT4135 - Assignment 1 - Martin Eggen & Martin Færevaag\n",
    "\n",
    "## Problem 1 Information theory and data compression\n",
    "\n",
    "This problem set addresses information theory and lossless data compression. The first\n",
    "set of problems are theoretical only:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3829b5",
   "metadata": {},
   "source": [
    "## 1a)\n",
    "**Find the entropy for the following distributions:**\n",
    "- $P(X=x) = 1/2$ for  $X\\in \\{0,1\\}$\n",
    "- $P(X=0)=0, P(X=x) = 1/2$ for  $X\\in \\{1,2\\}$\n",
    "- $P(X=0, Y=0) = 1/2, P(X = 0, Y = 1) = 1/4, P(X = 1, Y = 0) = 1/8, P(X = 1, Y = 1) = 1/8$\n",
    "- $P(X=n)= 2^n, n\\in\\{1,2,3\\ldots,\\infty\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62e99d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1a I) The entropy is 1.0\n",
      "1a II) The entropy is 1.0\n",
      "1a III) The entropy is 1.75\n",
      "1a IV) Then entropy is 2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#1a I\n",
    "p = 1/2\n",
    "H = -p*np.log2(p)-(1-p)*np.log2(1-p)\n",
    "print(\"1a I) The entropy is\", H)\n",
    "\n",
    "# 1a II\n",
    "p = np.array([1/2, 1/2]) #Ignore 0 because it is 0 chance\n",
    "H = np.sum(-p*np.log2(p))\n",
    "print(\"1a II) The entropy is\", H)\n",
    "\n",
    "#1a III\n",
    "p = np.array([1/2,1/4,1/8,1/8])\n",
    "H = np.sum(-p*np.log2(p))\n",
    "print(\"1a III) The entropy is\", H)\n",
    "\n",
    "#1a IV\n",
    "n = 1\n",
    "run = True\n",
    "ssum = 0\n",
    "while run:\n",
    "    verdi = -2**(-n)*np.log2(2**(-n))\n",
    "    ssum = ssum + verdi\n",
    "    n+=1\n",
    "    if abs(verdi) <= 0.000001:\n",
    "        run = False\n",
    "print(\"1a IV) Then entropy is\", round(ssum,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53a4b5c",
   "metadata": {},
   "source": [
    "## 1b)\n",
    "Let $X,Y \\in \\{0,1\\}$. Let $P(Y=0 = 1/4, P(X=0|Y=0) = 1/2$ and $P(X=1|Y=1)=1/2$. Compute  $H(X), H(Y), H(X,Y), H(X|Y), H(Y|X)$ and $I(X;Y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29338930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(X) =  1.0\n",
      "H(Y) =  0.8112781244591328\n",
      "H(X,Y) =  1.811278124459133\n",
      "H(X|Y) =  1.0\n",
      "H(Y|X) =  0.8112781244591329\n",
      "I(X;Y) =  0.0\n"
     ]
    }
   ],
   "source": [
    "P_Y = np.array([1/4, 1-1/4])\n",
    "P_XgivenY = np.array([[1/2,1/2], # Rows: X=x, Columns Y=y\n",
    "                        [1/2,1/2]])\n",
    "P_X = P_XgivenY.dot(P_Y)\n",
    "\n",
    "P_XandY = list()\n",
    "P_XandY_log = list()\n",
    "P_YgivenX = np.empty(shape=(2,2))\n",
    "I_log = list()\n",
    "\n",
    "for i in range(0,2):\n",
    "    for j in range(0,2):\n",
    "        P_XandY.append(P_X[i]*P_Y[j])\n",
    "        P_XandY_log.append(P_X[j]*P_Y[i])\n",
    "        temp = P_XgivenY[i,j]*P_Y[j]/P_X[i]\n",
    "        P_YgivenX[i,j] = temp\n",
    "        I_log.append(P_X[i]*P_Y[j]/(P_X[i]*P_Y[j]))\n",
    "        \n",
    "HX = -np.sum(P_X*np.log2(P_X))\n",
    "HY = -np.sum(P_Y*np.log2(P_Y))\n",
    "HXandY = -np.sum(np.array(P_XandY)*np.log2(np.array(P_XandY)))\n",
    "HXgivenY = -np.sum(np.array(P_XandY)*np.log2(P_XgivenY.flatten()))\n",
    "HYgivenX = -np.sum(np.array(P_XandY)*np.log2(P_YgivenX.flatten()))\n",
    "IXY = HX - HXgivenY\n",
    "\n",
    "print(\"H(X) = \",HX)\n",
    "print(\"H(Y) = \",HY)\n",
    "print(\"H(X,Y) = \",HXandY)\n",
    "print(\"H(X|Y) = \",HXgivenY)\n",
    "print(\"H(Y|X) = \",HYgivenX)\n",
    "print(\"I(X;Y) = \",IXY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd1fbf9",
   "metadata": {},
   "source": [
    "## 1c) \n",
    "(Hard) A Markov chain is a sequence $X1, X2, X3,\\ldots$ of random variables that has the following property:\n",
    "\n",
    "\\begin{align}\n",
    "P(Xn|X_{n-1}, X_{n-2}, X_{n-3}, \\ldots) = P(Xn|Xn−1)\n",
    "\\end{align}\n",
    "for all *n*. We can describe a Markov chain completely by its initial probabilities $P(X1)$ and\n",
    "transition probabilities $P(Xn|Xn−1)$. Let $X ∈ \\{0, 1\\}$ and\n",
    "\n",
    "\\begin{align}\n",
    "P(X = 0) = 1/2 \\\\\n",
    "P(X_{n} = 0|X_{n-2} = 0) = 7/8 \\\\\n",
    "P(X_{n} = 1|X_{n-1} = 1) = 7/8\n",
    "\\end{align}\n",
    "Find the steady state distribution for X, which is the probability $P(Xn = x)$ for any *n*. Then\n",
    "compute the entropy rate of the sequence $X1, X2, X3, \\ldots, Xn$ as *n* goes to infinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "515b00bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stationary_distribution(transition_matrix):\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(transition_matrix.T)\n",
    "    stationary_distribution = eigenvectors[:, np.isclose(eigenvalues, 1)].flatten()\n",
    "    return stationary_distribution/np.sum(stationary_distribution)\n",
    "\n",
    "def get_entropy_rate(transition_matrix):\n",
    "    steady_state = get_stationary_distribution(transition_matrix)\n",
    "    entropy_rate = 0\n",
    "    for i in range(0, len(steady_state)):\n",
    "        for j in range(0, len(steady_state)):\n",
    "            entropy_rate += steady_state[i]*transition_matrix[i,j]*np.log2(transition_matrix[i,j]) if transition_matrix[i,j] != 0 else 0\n",
    "    return -entropy_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de8f7f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stady state distribution is:  [0.5 0.5]\n",
      "entropy rate is:  0.5435644431995964\n"
     ]
    }
   ],
   "source": [
    "transition_matrix = np.array([[7/8,1/8],[1/8,7/8]])\n",
    "steady_state_distribution = get_stationary_distribution(transition_matrix)\n",
    "\n",
    "print(\"stady state distribution is: \",steady_state_distribution)\n",
    "print(\"entropy rate is: \", get_entropy_rate(transition_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0884ab7d",
   "metadata": {},
   "source": [
    "The next problems involve numerical exercises using Python. There are three data files\n",
    "named _data[123].txt_ that shall be used for the problems below. They are all text files with a\n",
    "single 0 or 1 per line. You can easily load the files with _numpy.loadtxt_ (see NumPy manual)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df026418",
   "metadata": {},
   "source": [
    "## 1d) \n",
    "Estimate the entropy for each of the three files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a47746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(data, max_val):\n",
    "    symbols = dict()\n",
    "    prob = list()\n",
    "    for i in range(0, max_val+1):\n",
    "        symbols[bin(i)[2:].zfill(len(data[0]))] = 0\n",
    "    for i in data:\n",
    "        symbols[i] += 1\n",
    "    for key in symbols:\n",
    "        prob.append(symbols[key]/len(data))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb7ce63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(data1) =  0.8113854260218433\n",
      "H(data2) =  0.8112781244591328\n",
      "H(data3) =  0.8107562272490967\n"
     ]
    }
   ],
   "source": [
    "data1 = np.loadtxt(\"data1.txt\")\n",
    "data1 = [str(int(i)) for i in data1]\n",
    "data2 = np.loadtxt(\"data2.txt\")\n",
    "data2 = [str(int(i)) for i in data2]\n",
    "data3 = np.loadtxt(\"data3.txt\")\n",
    "data3 = [str(int(i)) for i in data3]\n",
    "\n",
    "datas = [data1, data2, data3]\n",
    "P_datas = list()\n",
    "for i in datas:\n",
    "    prob = [i for i in get_prob(i,1) if i != 0] \n",
    "    P_datas.append(prob)\n",
    "\n",
    "Hdatas = list()\n",
    "for i in P_datas:\n",
    "    Hdatas.append(-np.sum(i*np.log2(i)))\n",
    "\n",
    "print(\"H(data1) = \",Hdatas[0])\n",
    "print(\"H(data2) = \",Hdatas[1])\n",
    "print(\"H(data3) = \",Hdatas[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a959614",
   "metadata": {},
   "source": [
    "## 1e) \n",
    "Estimate the entropy for each of the three files when considering two, three and four bits\n",
    "at the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ef3f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_bits(file, num):\n",
    "    new_array = list()\n",
    "    for i in range(0, len(file), num):\n",
    "        try:\n",
    "            word = \"\"\n",
    "            for j in range(0, num):\n",
    "                word += str(int(file[i+j]))\n",
    "            new_array.append(word)\n",
    "        except:\n",
    "            pass\n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6b1348a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(data1_2bit) =  1.6227703722997504\n",
      "H(data2_2bit) =  1.4999998055104773\n",
      "H(data3_2bit) =  1.1344210738693024\n",
      "\n",
      "H(data1_3bit) =  2.4341526972373906\n",
      "H(data2_3bit) =  1.9999996312441641\n",
      "H(data3_3bit) =  1.460224165956228\n",
      "\n",
      "H(data1_4bit) =  3.245529693510311\n",
      "H(data2_4bit) =  0.9999996110209546\n",
      "H(data3_4bit) =  1.781939782450746\n"
     ]
    }
   ],
   "source": [
    "datas_2bit = list()\n",
    "for i in datas:\n",
    "    datas_2bit.append(merge_bits(i,2))\n",
    "\n",
    "P_datas_2bit = list()\n",
    "for i in datas_2bit:\n",
    "    prob = [i for i in get_prob(i,3) if i != 0] \n",
    "    P_datas_2bit.append(prob)\n",
    "\n",
    "Hdatas_2bit = list()\n",
    "for i in P_datas_2bit:\n",
    "    Hdatas_2bit.append(-np.sum(i*np.log2(i)))\n",
    "\n",
    "print(\"H(data1_2bit) = \",Hdatas_2bit[0])\n",
    "print(\"H(data2_2bit) = \",Hdatas_2bit[1])\n",
    "print(\"H(data3_2bit) = \",Hdatas_2bit[2])\n",
    "##########################################################\n",
    "datas_3bit = list()\n",
    "for i in datas:\n",
    "    datas_3bit.append(merge_bits(i,3))\n",
    "\n",
    "P_datas_3bit = list()\n",
    "for i in datas_3bit:\n",
    "    prob = [i for i in get_prob(i,7) if i != 0] \n",
    "    P_datas_3bit.append(prob)\n",
    "\n",
    "Hdatas_3bit = list()\n",
    "for i in P_datas_3bit:\n",
    "    Hdatas_3bit.append(-np.sum(i*np.log2(i)))\n",
    "\n",
    "print(\"\\nH(data1_3bit) = \",Hdatas_3bit[0])\n",
    "print(\"H(data2_3bit) = \",Hdatas_3bit[1])\n",
    "print(\"H(data3_3bit) = \",Hdatas_3bit[2])\n",
    "##########################################################\n",
    "datas_4bit = list()\n",
    "for i in datas:\n",
    "    datas_4bit.append(merge_bits(i,4))\n",
    "\n",
    "P_datas_4bit = list()\n",
    "for i in datas_4bit:\n",
    "    prob = [i for i in get_prob(i,15) if i != 0] \n",
    "    P_datas_4bit.append(prob)\n",
    "\n",
    "Hdatas_4bit = list()\n",
    "for i in P_datas_4bit:\n",
    "    Hdatas_4bit.append(-np.sum(i*np.log2(i)))\n",
    "\n",
    "print(\"\\nH(data1_4bit) = \",Hdatas_4bit[0])\n",
    "print(\"H(data2_4bit) = \",Hdatas_4bit[1])\n",
    "print(\"H(data3_4bit) = \",Hdatas_4bit[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a79b4b0",
   "metadata": {},
   "source": [
    "## 1f)\n",
    "Attempt to estimate the entropy rate for each of the three files. Note that you will need\n",
    "to approximate the rate by some finite symbols length n which is up to you to determine\n",
    "(Note: The choice of n is important as increasing n arbitrarily will lead to a entropy rate\n",
    "equal to zero. Why?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61d754b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy rate of data1 =  0.8112080039785136\n",
      "entropy rate of data2 =  0.34999823857974743\n",
      "entropy rate of data3 =  0.3724755547990849\n",
      "\n",
      "When increasing n arbitrariliy the entroyp rate goes to zero, becuase the number of possible states increases exponentially, but most of the states will have a probability of zero and will not contribute to the entropy, then you divide by n and the entropy rate goes to zero.\n"
     ]
    }
   ],
   "source": [
    "datas_10bit = list()\n",
    "for i in datas:\n",
    "    datas_10bit.append(merge_bits(i,10))\n",
    "\n",
    "P_datas_10bit = list()\n",
    "for i in datas_10bit:\n",
    "    prob = [i for i in get_prob(i,1023) if i != 0] \n",
    "    P_datas_10bit.append(prob)\n",
    "\n",
    "Hdatas_10bit = list()\n",
    "for i in P_datas_10bit:\n",
    "    Hdatas_10bit.append(-np.sum(i*np.log2(i)))\n",
    "\n",
    "print(\"entropy rate of data1 = \",Hdatas_10bit[0]/10)\n",
    "print(\"entropy rate of data2 = \",Hdatas_10bit[1]/10)\n",
    "print(\"entropy rate of data3 = \",Hdatas_10bit[2]/10)\n",
    "print(\"\\nWhen increasing n arbitrariliy the entroyp rate goes to zero, becuase the number of possible states increases exponentially, but most of the states will have a probability of zero and will not contribute to the entropy, then you divide by n and the entropy rate goes to zero.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c31eec",
   "metadata": {},
   "source": [
    "## 1g)\n",
    "Try and model each of the three data files as a Markov chain. Estimate the transition and\n",
    "steady state parameters and compute the entropy. Compare with the entropy rate from\n",
    "the previous problem. Which problem is likely to come from a binary Markov chain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "435b9032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_matrix(data):\n",
    "    symbols = dict()\n",
    "    max_val = 2**len(data[0])\n",
    "    for i in range(0, max_val):\n",
    "        symbols[bin(i)[2:].zfill(len(data[0]))] = dict()\n",
    "        for j in range(0, max_val):\n",
    "            symbols[bin(i)[2:].zfill(len(data[0]))][bin(j)[2:].zfill(len(data[0]))] = 0\n",
    "    for i in range(0, len(data)-1):\n",
    "        symbols[data[i]][data[i+1]] += 1\n",
    "    transition_matrix = np.zeros((max_val,max_val))\n",
    "    row = 0\n",
    "    for i in symbols:\n",
    "        column = 0\n",
    "        count = sum(symbols[i].values())\n",
    "        for j in symbols[i]:\n",
    "            transition_matrix[row,column] = symbols[i][j]/count if count != 0 else 0\n",
    "            column += 1\n",
    "        row += 1\n",
    "    return transition_matrix     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d09a8e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition matrix of data1 =\n",
      " [[0.24975116 0.75024884]\n",
      " [0.25017335 0.74982665]]\n",
      "\n",
      "transition matrix of data2 =\n",
      " [[0.         1.        ]\n",
      " [0.33333344 0.66666656]]\n",
      "\n",
      "transition matrix of data3 =\n",
      " [[0.87448577 0.12551423]\n",
      " [0.04176471 0.95823529]]\n",
      "\n",
      "steady state of data1 = [0.25006777 0.74993223]\n",
      "\n",
      "steady state of data2 = [0.25000006 0.74999994]\n",
      "\n",
      "steady state of data3 = [0.24967104 0.75032896]\n",
      "\n",
      "entropy rate of data1 = 0.8113853918886793\n",
      "\n",
      "entropy rate of data2 = 0.6887219002790084\n",
      "\n",
      "entropy rate of data3 = 0.32389885074769115\n"
     ]
    }
   ],
   "source": [
    "datas_transition_matrix = [get_transition_matrix(i) for i in datas]\n",
    "\n",
    "print(\"transition matrix of data1 =\\n\",datas_transition_matrix[0])\n",
    "print(\"\\ntransition matrix of data2 =\\n\",datas_transition_matrix[1])\n",
    "print(\"\\ntransition matrix of data3 =\\n\",datas_transition_matrix[2])\n",
    "\n",
    "print(\"\\nsteady state of data1 =\",get_stationary_distribution(datas_transition_matrix[0]))\n",
    "print(\"\\nsteady state of data2 =\",get_stationary_distribution(datas_transition_matrix[1]))\n",
    "print(\"\\nsteady state of data3 =\",get_stationary_distribution(datas_transition_matrix[2]))\n",
    "\n",
    "print(\"\\nentropy rate of data1 =\",get_entropy_rate(datas_transition_matrix[0]))\n",
    "print(\"\\nentropy rate of data2 =\",get_entropy_rate(datas_transition_matrix[1]))\n",
    "print(\"\\nentropy rate of data3 =\",get_entropy_rate(datas_transition_matrix[2])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d439f421",
   "metadata": {},
   "source": [
    "\n",
    "A run length code can be defined as follows. Given a bit sequence\n",
    "\\begin{align}\n",
    "\\\\\n",
    "S = 111001111100111110000\\\\\n",
    "\\end{align}\n",
    "\n",
    "we can code it as\n",
    "\n",
    "\\begin{align}\n",
    "\\\\\n",
    "C = 0325254\\\\\n",
    "\\end{align}\n",
    "\n",
    "which can be interpreted as “zero 0s, three 1s, 2 0s, 5 1s, ...” etc. We have assumed that\n",
    "we always start counting the zeros (here there are zero of them). We also need to determine\n",
    "what the maximum length we can represent is, and how we will represent sequences longer\n",
    "than this. Here we use the convention that if a sequence of eg. zeros are larger than some\n",
    "maximum N, we write the code as N0M where M is the remainder of the symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28bddbd",
   "metadata": {},
   "source": [
    "## 1h)\n",
    "Implement a run-length code in Python and code the file _data3.txt_, using\n",
    "\\begin{align}\n",
    "\\\\\n",
    "N ∈ \\{7, 15, 31, 63, 127\\}\\\\\n",
    "\\end{align}\n",
    "\n",
    "What is the coding gain for those three cases? How does it compare with the entropy\n",
    "rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7f61be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_length_encoder(data, n):\n",
    "    bit_length = int(np.log2(n+1))\n",
    "    bol = False\n",
    "    encoded = \"\"\n",
    "    count = 0\n",
    "    i = 0\n",
    "    while i < len(data):\n",
    "        if count==n:\n",
    "            encoded += bin(count)[2:].zfill(bit_length)\n",
    "            count = 0\n",
    "        elif data[i] != str(int(bol)):\n",
    "            encoded += bin(count)[2:].zfill(bit_length)\n",
    "            count = 0\n",
    "            bol = not bol\n",
    "        else:\n",
    "            count += 1\n",
    "            i += 1\n",
    "    encoded += bin(count)[2:].zfill(bit_length)\n",
    "    if count==n:\n",
    "        encoded += bin(0)[2:].zfill(bit_length)\n",
    "    return encoded\n",
    "\n",
    "def run_length_decoder(data, n):\n",
    "    bit_length = int(np.log2(n+1))\n",
    "    bol = False\n",
    "    decoded = \"\"\n",
    "    for i in range(0, len(data), bit_length):\n",
    "        decimal = int(data[i:i+bit_length],2)\n",
    "        decoded += str(int(bol))*decimal\n",
    "        if decimal != n:\n",
    "            bol = not bol\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01990239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_length_encoder_noob(data, n):\n",
    "    bit_length = int(np.log2(n+1))\n",
    "    bol = False\n",
    "    encoded = \"\"\n",
    "    count = 0\n",
    "    i = 0\n",
    "    while i < len(data):\n",
    "        if count==n:\n",
    "            encoded += bin(count)[2:].zfill(bit_length)\n",
    "            count = 0\n",
    "            bol = not bol\n",
    "        elif data[i] != str(int(bol)):\n",
    "            encoded += bin(count)[2:].zfill(bit_length)\n",
    "            count = 0\n",
    "            bol = not bol\n",
    "        else:\n",
    "            count += 1\n",
    "            i += 1\n",
    "    encoded += bin(count)[2:].zfill(bit_length)\n",
    "    return encoded\n",
    "\n",
    "def run_length_decoder_noob(data, n):\n",
    "    bit_length = int(np.log2(n+1))\n",
    "    bol = False\n",
    "    decoded = \"\"\n",
    "    for i in range(0, len(data), bit_length):\n",
    "        decimal = int(data[i:i+bit_length],2)\n",
    "        decoded += str(int(bol))*decimal\n",
    "        bol = not bol\n",
    "    return decoded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a003fb7",
   "metadata": {},
   "source": [
    "## 1i)\n",
    "The file _data4.bin_ contains a version of the binary data from _data3.txt_ where the bits\n",
    "have been packed into bytes and stored as a binary file. Use any compression program\n",
    "of you choice (Zip, GZip, BZip2, 7Zip etc.) to compress the file and compare the results\n",
    "with the entropy rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c309da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N = 7 :\n",
      "encoded data3 size =  2260011\n",
      "encoded data3 gain fast =  0.5388286113739014\n",
      "\n",
      "N = 15 :\n",
      "encoded data3 size =  1755900\n",
      "encoded data3 gain fast =  0.4186391830444336\n",
      "\n",
      "N = 31 :\n",
      "encoded data3 size =  1576290\n",
      "encoded data3 gain fast =  0.37581682205200195\n",
      "\n",
      "N = 63 :\n",
      "encoded data3 size =  1637850\n",
      "encoded data3 gain fast =  0.39049386978149414\n",
      "\n",
      "N = 127 :\n",
      "encoded data3 size =  1844794\n",
      "encoded data3 gain fast =  0.4398331642150879\n",
      "\n",
      "By using 7Zip, the file got compressed down to 196 kB = 1568000 bits\n",
      "That is a compression ratio of 196/516 KB = 0.38\n",
      "The best compression ratio was acheived by using N = 31\n",
      "Our code made it only 0.5% bigger than what 7Zip did\n"
     ]
    }
   ],
   "source": [
    "N = [7, 15, 31, 63, 127]\n",
    "\n",
    "for i in N:\n",
    "    encoded_data3_fast = run_length_encoder(datas[2], i)\n",
    "    print(f\"\\nN = {i} :\")\n",
    "    print(f\"encoded data3 size = \",len(encoded_data3_fast))\n",
    "    print(f\"encoded data3 gain fast = \",len(encoded_data3_fast)/len(datas[2]))\n",
    "    \n",
    "print(\"\\nBy using 7Zip, the file got compressed down to 196 kB = 1568000 bits\")\n",
    "print(\"That is a compression ratio of 196/516 KB = 0.38\")\n",
    "\n",
    "print(\"The best compression ratio was acheived by using N = 31\")\n",
    "print(\"Our code made it only 0.5% bigger than what 7Zip did\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413d8d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = [7, 15, 31, 63, 127]\n",
    "\n",
    "for i in N:\n",
    "    encoded_data3_fast = run_length_encoder_noob(datas[2], i)\n",
    "    print(f\"\\nN = {i} :\")\n",
    "    print(f\"encoded data3 size = \",len(encoded_data3_fast))\n",
    "    print(f\"encoded data3 gain fast = \",len(encoded_data3_fast)/len(datas[2]))\n",
    "\n",
    "decoded_data3_fast = run_length_decoder_noob(encoded_data3_fast, 127)\n",
    "\n",
    "for i in range(datas[2]):\n",
    "    if i != decoded_data3_fast[i]:\n",
    "        print(\"error\")\n",
    "        break\n",
    "else:\n",
    "    print(\"no error\")\n",
    "\n",
    "    \n",
    "print(\"\\nBy using 7Zip, the file got compressed down to 196 kB = 1568000 bits\")\n",
    "print(\"That is a compression ratio of 196/516 KB = 0.38\")\n",
    "\n",
    "print(\"The best compression ratio was acheived by using N = 31\")\n",
    "print(\"Our code made it only 0.5% bigger than what 7Zip did\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb4a53e",
   "metadata": {},
   "source": [
    "## Problem 2 Deep neural networks\n",
    "This problem is designed to be very open and is aiming to be similar to the process of\n",
    "training a deep neural network from scratch.\\\n",
    "\\\n",
    "You will find the file _problem2_template.py_ attached. This file contains a bare bones script\n",
    "for training a very simple classifier on the **CIFAR 10** datasets (_https://www.cs.toronto.edu/\n",
    "~kriz/cifar.html_). This is a classification task that has ten classes - **’plane’, ’car’, ’bird’, ’cat’,\n",
    "’deer’, ’dog’, ’frog’, ’horse’, ’ship’ and ’truck’**.\\\n",
    "We want to you create a DNN classifier and use it to do image retrieval on the test set. For\n",
    "example, if you decide to retrieve images of dogs, you should measure the accuracy, precision\n",
    "and recall for this task. We also want you to use the AUPRC (area under the PR curve) to\n",
    "rank the different systems you make.\\\n",
    "In your task you are free to add new layers, increase the number of nodes per layer, add\n",
    "dropout and change the optimization algorithm if so be. The dataset is small, but the training\n",
    "will take some time, so you will need to make sure you pick the lowest hanging fruits first.\n",
    "The answer to this problem should of course be the precision-recall curve for the retrival\n",
    "problem, but also a description of how you improved the original neural network with the\n",
    "relevant results in tables and graphs.\\\n",
    "\\\n",
    "As a starting point you should to the following:\n",
    "1. Run the script and make sure it works. You will need the two Python modules *torch*\n",
    "and *torchvision*. Make a note of the time it takes to finish the training, and adjust the\n",
    "number of epochs to make your turn-around between experiments quick enough.\n",
    "2. Divide the training set into a training and validation set. You can do this manually or\n",
    "using tools from *scikit-learn*.\n",
    "3. Add layers to the neural network and observe the performance on the validation set.\n",
    "4. Use *scikit-learn* to compute PR curves for the retrieval problem and compare the\n",
    "results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f17acc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[1] loss: 1.732\n",
      "[2] loss: 1.367\n",
      "[3] loss: 1.235\n",
      "[4] loss: 1.146\n",
      "[5] loss: 1.075\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 59 %\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                     (0.5, 0.5, 0.5))])\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "kwargs = {} if device=='cpu' else {'num_workers': 1, 'pin_memory': True}\n",
    "batch_size=4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, **kwargs)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, **kwargs)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        # inputs, labels = data\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    print('[%d] loss: %.3f' %\n",
    "            (epoch + 1, running_loss / i))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        # images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%'\n",
    "      % (100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a710750a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "🍔🍔🍔🍔🍔🍔🍔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d390286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd06c82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9220ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe8793d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "835a0a66f5e0e0b5327b638cecd9416a1b95bb1147974c9b466cb34bc814813a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
